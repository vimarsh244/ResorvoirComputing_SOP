\clearpage
\appendix
\section{Code}

The complete code with different tests and experiments conducted is available on \href{https://github.com/vimarsh244/ResorvoirComputing_SOP}{GitHub}.

\subsection*{Predicting Lorentz System Evolution using Pendulum model}
\label{code_lorenz_pendulum}
Generating Lorenz system data and training a reservoir computing model
\begin{minted}{python}
# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from scipy.integrate import odeint
from reservoirpy.nodes import Reservoir
from reservoirpy.nodes import Ridge

# Define the Lorenz system
def lorenz_system(state, t):
    x, y, z = state
    dx = 10 * (y - x)
    dy = x * (28 - z) - y
    dz = x * y - (8/3) * z
    return [dx, dy, dz]

# Generate time series data
t_lorenz = np.linspace(0, 100, 10000)
initial_state = [1, 1, 1]
solution = odeint(lorenz_system, initial_state, t_lorenz)

# Prepare input (x) and output (full state) data
X_lorenz = solution[:-1, 0].reshape(-1, 1)  # Input: x component
y_lorenz = solution[1:, :]                 # Target: (x, y, z)

# Configure reservoir model
reservoir = Reservoir(
    units=500,
    lr=0.3,
    sr=0.9,
    input_scaling=0.5,
    bias_scaling=0.1
)
readout = Ridge(ridge=1e-6)
model = reservoir >> readout  # Connect reservoir to readout

# Train the model
model.fit(X_lorenz, y_lorenz)

# Predict on initial segment
predicted = model.run(X_lorenz[:2000])

# Plot true vs predicted Lorenz trajectories
plt.figure(figsize=(12, 6))
plt.plot(t_lorenz[:2000], y_lorenz[:2000, 1], 'b-', label='Actual $y(t)$')
plt.plot(t_lorenz[:2000], predicted[:, 1], 'r--', label='Predicted $y(t)$')
plt.xlabel('Time')
plt.ylabel('$y(t)$')
plt.legend()
plt.title('Lorenz System: Actual vs Predicted $y(t)$')

# 3D Plot of true and predicted trajectories
fig = plt.figure(figsize=(15, 12))
ax = fig.add_subplot(111, projection='3d')
ax.plot3D(solution[100:-100, 0], solution[100:-100, 1], solution[100:-100, 2],
          lw=0.5, c='darkblue', alpha=0.8, label='True trajectory')
ax.plot3D(predicted[100:-100, 0], predicted[100:-100, 1], predicted[100:-100, 2],
          lw=0.5, c='red', linestyle='--', alpha=0.8, label='Predicted trajectory')
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')
ax.set_title('Lorenz Attractor: True vs Predicted')
ax.legend()
ax.view_init(elev=20, azim=45)
ax.grid(False)
plt.tight_layout()
\end{minted}


\subsection*{Polynomial Prediction using Logistic Map}\label{code_logistic_polynomial}
Generating a 7th degree polynomial system data and training a reservoir computing model using the logistic map equation
\begin{minted}{python}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error

# -----------------------------------
# the target polynomial f(x)
#    f(x) = (x-3)(x-2)(x-1)x(x+1)(x+2)(x+3)
# -----------------------------------
def poly7(x):
    return (x-3)*(x-2)*(x-1)*x*(x+1)*(x+2)*(x+3)

# sample L points uniformly in [-3.5, 3.5]
L = 2000
u = np.linspace(-3.5, 3.5, L).reshape(-1,1)
v = poly7(u).ravel()  # shape (L,)

# split into train/test
frac_train = 0.7
n_train = int(frac_train * L)
u_train, u_test = u[:n_train], u[n_train:]
v_train, v_test = v[:n_train], v[n_train:]


def build_logistic_reservoir(u, P=500, amin=3.6, amax=4.0, init=0.5):
    """
    Maps each scalar input u[j] to parameter a_j in [amin,amax],
    iterates logistic map x_{k+1} = a_j * x_k * (1 - x_k) for P steps,
    starting from x_0 = init. Returns reservoir matrix R of shape
    (len(u), P).
    """
    N = len(u)
    R = np.zeros((N, P))
    umin, umax = u.min(), u.max()
    for j in range(N):
        # linear map input -> logistic parameter
        a_j = amin + (amax - amin) * (u[j,0] - umin) / (umax - umin)
        x = init
        for k in range(P):
            x = a_j * x * (1 - x)
            R[j, k] = x
    return R

# build reservoir for train and test
P = 100
R_train = build_logistic_reservoir(u_train, P=P)
R_test  = build_logistic_reservoir(u_test,  P=P)

ridge = Ridge(alpha=1e-8)
ridge.fit(R_train, v_train)

# predict on test set
v_pred = ridge.predict(R_test)

# compute benchmark: RMSE
rmse = np.sqrt(mean_squared_error(v_test, v_pred))
print(f"Polynomial task RMSE : {rmse:.4e}")

plt.figure(figsize=(6,4))
plt.scatter(u_test, v_test,  color='navy', s=20, label='True f(u)')
plt.scatter(u_test, v_pred, color='crimson', s=20, marker='x',
            label='RC-predicted')
plt.xlabel('u')
plt.ylabel('f(u)')
plt.title('7th-order Polynomial: True vs RC Prediction')
plt.legend()
plt.tight_layout()
plt.show()

\end{minted}
% \end{lstlisting}


\subsection*{Lorentz System Prediction using Logistic Map} \label{code_logistic_lorenz}
Generating a Lorenz system data and training a reservoir computing model using the logistic map equation
\begin{minted}{python}

import numpy as np
import matplotlib.pyplot as plt
from scipy.integrate import odeint
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error


def lorenz_system(state, t, sigma=10.0, rho=28.0, beta=8/3):
    x, y, z = state
    return [
        sigma * (y - x),
        x * (rho - z) - y,
        x * y - beta * z
    ]

T = 100.0
N = 10000
t = np.linspace(0, T, N)
x0 = [1.0, 1.0, 1.0]
data = odeint(lorenz_system, x0, t)

# inputs u[n] = x[n], targets y[n] = [x[n+1], y[n+1], z[n+1]]
U = data[:-1, 1].reshape(-1, 1)   # shape (N-1,1)
Y = data[1:, :]                  # shape (N-1,3)

# split into train / test
train_frac = 0.8
split = int(len(U) * train_frac)
U_train, U_test = U[:split], U[split:]
Y_train, Y_test = Y[:split], Y[split:]

def build_logistic_reservoir(u, P=200, amin=3.6, amax=4.0, init=0.5):
    N = len(u)
    R = np.zeros((N, P))
    umin, umax = u.min(), u.max()
    for j in range(N):
        a_j = amin + (amax - amin) * (u[j,0] - umin) / (umax - umin)
        x = init
        for k in range(P):
            x = a_j * x * (1 - x)
            R[j, k] = x
    return R

R_train = build_logistic_reservoir(U_train)
R_test  = build_logistic_reservoir(U_test)


ridge = Ridge(alpha=1e-6)
ridge.fit(R_train, Y_train)

# predict on test set
Y_pred = ridge.predict(R_test)

# compute benchmark: RMSE per component
rmse = np.sqrt(mean_squared_error(Y_test, Y_pred, multioutput='raw_values'))
print(f"Test RMSE (x,y,z): {rmse}")


# Time-series: compare y-component
t_test = t[1+split:1+split+500]  # first 500 test points
plt.figure(figsize=(8,4))
plt.plot(t_test, Y_test[:500,1], 'b',  label='True y(t)')
plt.plot(t_test, Y_pred[:500,1],'r--',label='Pred y(t)')
plt.xlabel('Time')
plt.ylabel('y')
plt.title('Lorenz: True vs Predicted y(t) (first 500 pts)')
plt.legend()
plt.tight_layout()

# 2D projections of the attractor: (x vs y), (x vs z), (y vs z)
fig, axes = plt.subplots(1, 3, figsize=(15,5))
pairs = [ (0,1,'x','y'),
          (0,2,'x','z'),
          (1,2,'y','z')]
start, end = split+100, split+1100  # subset for clarity

for ax, (i,j,xi,yj) in zip(axes, pairs):
    ax.scatter(data[start:end,i],
               data[start:end,j],
               c='navy', s=1, label='True')
    ax.scatter(Y_pred[start-split:end-split,i],
               Y_pred[start-split:end-split,j],
               c='crimson', s=1, marker='x', label='Pred')
    ax.set_xlabel(xi); ax.set_ylabel(yj)
    ax.legend(loc='upper right')
    ax.set_title(f'{xi} vs {yj}')

fig.suptitle('Lorenz Attractor Projections: True (blue) vs Predicted (red)', y=1.02)
plt.tight_layout()
plt.show()
\end{minted}

\subsection*{Prediction of Bifurcation Diagram} \label{code_elm_bd}
\begin{minted}{python}
# %%
import numpy as np
import matplotlib.pyplot as plt

# Fixing random seed for reproducibility
np.random.seed(42)

# Simulating the true logistic map to generate the bifurcation diagram
r_vals = np.linspace(2.5, 4.5, 401)  # parameter range (extended)
true_r = []
true_x = []
for r in r_vals:
    x = 0.5
    # Discard transients
    for _ in range(100):
        x = r * x * (1 - x)
    # Record subsequent values for bifurcation plot
    for _ in range(100):
        x = r * x * (1 - x)
        true_r.append(r)
        true_x.append(x)
true_r = np.array(true_r)
true_x = np.array(true_x)
# Filter out invalid values (for r>4, the logistic map overflows)
mask = np.isfinite(true_x)
true_r = true_r[mask]
true_x = true_x[mask]


# %%
# Defining and training the Echo State Network (ESN)
n_res = 200            # Reservoir size
input_dim = 2          # [x, r] as inputs
output_dim = 1         # predict next x

# Construct a sparse reservoir with specified spectral radius
sparsity = 0.1
W_res = np.random.rand(n_res, n_res) - 0.5
mask = (np.random.rand(n_res, n_res) < sparsity)
W_res *= mask
# Scale to desired spectral radius
eigvals = np.linalg.eigvals(W_res)
W_res *= (0.95 / np.max(np.abs(eigvals)))

# Input weight matrix (random)
W_in = (np.random.rand(n_res, input_dim) - 0.5) * 1.0

# Prepare training data from logistic map with r in [3.4, 3.8]
r_train_vals = np.linspace(3.4, 3.8, 21)
T_skip = 100
T_train = 500

states_list = []
x_in_list = []
r_in_list = []
x_out_list = []

for r in r_train_vals:
    x = np.random.rand()
    s = np.zeros(n_res)
    # Skip initial transients (teacher-forced update)
    for _ in range(T_skip):
        u = np.array([x, r])
        s = np.tanh(W_res.dot(s) + W_in.dot(u))
        x = r * x * (1 - x)
    # Collect reservoir states and targets
    for _ in range(T_train):
        u = np.array([x, r])
        s = np.tanh(W_res.dot(s) + W_in.dot(u))
        states_list.append(s.copy())
        x_in_list.append(x)
        r_in_list.append(r)
        x_next = r * x * (1 - x)
        x_out_list.append(x_next)
        x = x_next

states_train = np.array(states_list).T  # shape (n_res, N_total)
x_in_train = np.array(x_in_list)
r_in_train = np.array(r_in_list)
y_train = np.array(x_out_list)

# Build design matrix [reservoir_states; x; r; 1]
ones = np.ones_like(x_in_train)
X_design = np.vstack([states_train,
                      x_in_train.reshape(1,-1),
                      r_in_train.reshape(1,-1),
                      ones.reshape(1,-1)])

# Train output weights via ridge regression
reg = 1e-8
XTX = X_design.dot(X_design.T)
Id = np.eye(X_design.shape[0])
W_out = (y_train.reshape(1,-1).dot(X_design.T)).dot(np.linalg.inv(XTX + reg * Id))


# %%
# Using the trained ESN to reconstruct (predict) bifurcation over extended range
r_pred_vals = np.linspace(2.5, 4.5, 201)
T_skip_pred = 100
T_gen = 500

pred_r = []
pred_x = []

for r in r_pred_vals:
    x = np.random.rand()
    s = np.zeros(n_res)
    # Skip to warm up ESN (if r <= 4, use true logistic; if r > 4, use ESN prediction to avoid overflow)
    if r <= 4.0:
        for _ in range(T_skip_pred):
            u = np.array([x, r])
            s = np.tanh(W_res.dot(s) + W_in.dot(u))
            x = r * x * (1 - x)
    else:
        for _ in range(T_skip_pred):
            u = np.array([x, r])
            s = np.tanh(W_res.dot(s) + W_in.dot(u))
            ext = np.concatenate((s, [x, r, 1.0]))
            x = float(W_out.dot(ext))
    # Generate predictions (free-run)
    for _ in range(T_gen):
        u = np.array([x, r])
        s = np.tanh(W_res.dot(s) + W_in.dot(u))
        ext = np.concatenate((s, [x, r, 1.0]))
        x = float(W_out.dot(ext))
        pred_r.append(r)
        pred_x.append(x)

pred_r = np.array(pred_r)
pred_x = np.array(pred_x)
# Filter out non-finite predictions
mask_pred = np.isfinite(pred_x)
pred_r = pred_r[mask_pred]
pred_x = pred_x[mask_pred]


# %%
# Generating return-plot data for representative cases r=3.5 (periodic) and r=3.8 (chaotic)
r_periodic = 3.5
r_chaotic = 3.8
x0 = 0.2
trans = 100
series_len = 5000

# True logistic series (after transients)
def logistic_series(r, x0, skip, length):
    x = x0
    for _ in range(skip):
        x = r * x * (1 - x)
    series = [x]
    for _ in range(length):
        x = r * x * (1 - x)
        series.append(x)
    return np.array(series)

series_true_periodic = logistic_series(r_periodic, x0, trans, series_len)
series_true_chaotic  = logistic_series(r_chaotic,  x0, trans, series_len)

# ESN-predicted series (using the trained network)
def esn_series(r, x0, skip, length):
    x = x0
    s = np.zeros(n_res)
    # Warm-up with true logistic to initialize x
    for _ in range(skip):
        u = np.array([x, r])
        s = np.tanh(W_res.dot(s) + W_in.dot(u))
        x = r * x * (1 - x)
    series = []
    for _ in range(length):
        u = np.array([x, r])
        s = np.tanh(W_res.dot(s) + W_in.dot(u))
        ext = np.concatenate((s, [x, r, 1.0]))
        x = float(W_out.dot(ext))
        series.append(x)
    return np.array(series)

series_esn_periodic = esn_series(r_periodic, x0, trans, series_len)
series_esn_chaotic  = esn_series(r_chaotic,  x0, trans, series_len)


# %%
# Computing approximate Lyapunov exponents for true vs. ESN trajectories
r_lya_vals = np.linspace(2.5, 4.0, 51)
lyap_true = []
lyap_esn = []

for r in r_lya_vals:
    # True logistic Lyapunov
    x = 0.5
    for _ in range(100):
        x = r * x * (1 - x)
    vals = []
    for _ in range(500):
        vals.append(np.log(abs(r * (1 - 2*x))))
        x = r * x * (1 - x)
    lyap_true.append(np.mean(vals))
    # ESN series Lyapunov (skip as above)
    x = 0.5
    s = np.zeros(n_res)
    for _ in range(100):
        u = np.array([x, r])
        s = np.tanh(W_res.dot(s) + W_in.dot(u))
        x = r * x * (1 - x)
    vals2 = []
    for _ in range(500):
        u = np.array([x, r])
        s = np.tanh(W_res.dot(s) + W_in.dot(u))
        ext = np.concatenate((s, [x, r, 1.0]))
        x = float(W_out.dot(ext))
        vals2.append(np.log(abs(r * (1 - 2*x))))
    lyap_esn.append(np.mean(vals2))

lyap_true = np.array(lyap_true)
lyap_esn  = np.array(lyap_esn)
# Filter out any non-finite values (if any divergence)
finite_mask = np.isfinite(lyap_esn)
r_lya_vals = r_lya_vals[finite_mask]
lyap_true = lyap_true[finite_mask]
lyap_esn  = lyap_esn[finite_mask]


# %%
# Plotting

# Bifurcation diagram (true vs. ESN)
plt.figure(figsize=(8,6))
plt.scatter(true_r, true_x, s=0.1, color='black', label='True logistic')
plt.scatter(pred_r, pred_x, s=0.1, color='red', label='ESN reconstructed')
plt.xlim(2.5, 4.5)
plt.ylim(0, 1)
plt.xlabel('Parameter r')
plt.ylabel('State x')
plt.title('Logistic Map Bifurcation (True vs ESN)')
plt.legend()
plt.tight_layout()
plt.show()

# Return plots: (x_t vs x_{t+1}) for periodic and chaotic cases
fig, axes = plt.subplots(2, 2, figsize=(8,8))
# True periodic (r=3.5)
axes[0,0].scatter(series_true_periodic[:-1], series_true_periodic[1:], s=5, color='blue', alpha=0.5)
axes[0,0].set_title(f'True Return Plot (r={r_periodic})')
axes[0,0].set_xlabel('x(n)')
axes[0,0].set_ylabel('x(n+1)')
axes[0,0].set_xlim(0,1); axes[0,0].set_ylim(0,1)
# ESN periodic
axes[0,1].scatter(series_esn_periodic[:-1], series_esn_periodic[1:], s=5, color='orange', alpha=0.5)
axes[0,1].set_title(f'ESN Return Plot (r={r_periodic})')
axes[0,1].set_xlabel('x(n)')
axes[0,1].set_ylabel('x(n+1)')
axes[0,1].set_xlim(0,1); axes[0,1].set_ylim(0,1)
# True chaotic (r=3.8)
axes[1,0].scatter(series_true_chaotic[:-1], series_true_chaotic[1:], s=5, color='blue', alpha=0.5)
axes[1,0].set_title(f'True Return Plot (r={r_chaotic})')
axes[1,0].set_xlabel('x(n)')
axes[1,0].set_ylabel('x(n+1)')
axes[1,0].set_xlim(0,1); axes[1,0].set_ylim(0,1)
# ESN chaotic
axes[1,1].scatter(series_esn_chaotic[:-1], series_esn_chaotic[1:], s=5, color='orange', alpha=0.5)
axes[1,1].set_title(f'ESN Return Plot (r={r_chaotic})')
axes[1,1].set_xlabel('x(n)')
axes[1,1].set_ylabel('x(n+1)')
axes[1,1].set_xlim(0,1); axes[1,1].set_ylim(0,1)
plt.tight_layout()
plt.show()

# Lyapunov exponents comparison
plt.figure(figsize=(8,5))
plt.plot(r_lya_vals, lyap_true, label='True logistic', color='blue')
plt.plot(r_lya_vals, lyap_esn,  label='ESN reconstructed', color='orange')
plt.axhline(0, color='gray', linestyle='--')
plt.xlabel('Parameter r')
plt.ylabel('Lyapunov Exponent')
plt.title('Lyapunov Exponents (True vs ESN)')
plt.legend()
plt.tight_layout()
plt.show()
\end{minted}