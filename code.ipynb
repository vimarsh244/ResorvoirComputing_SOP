{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset built. Reservoir state dimension: 1000\n",
      "Epoch 50/500, Loss: 1225.786621\n",
      "Epoch 100/500, Loss: 1030.076294\n",
      "Epoch 150/500, Loss: 888.078125\n",
      "Epoch 200/500, Loss: 779.193848\n",
      "Epoch 250/500, Loss: 695.286133\n",
      "Epoch 300/500, Loss: 631.187927\n",
      "Epoch 350/500, Loss: 582.851318\n",
      "Epoch 400/500, Loss: 546.890869\n",
      "Epoch 450/500, Loss: 520.446289\n",
      "Epoch 500/500, Loss: 501.133423\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (100x500 and 1000x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 195\u001b[0m\n\u001b[1;32m    192\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 195\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 181\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# Evaluate the trained model on the test set using MSE loss\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 181\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_features\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    182\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((predictions \u001b[38;5;241m-\u001b[39m y_test) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[1], line 148\u001b[0m, in \u001b[0;36mLinearReadout.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (100x500 and 1000x1)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Physical parameters\n",
    "g = 9.81         # gravitational acceleration\n",
    "l = 1.0          # pendulum length\n",
    "k_damp = 0.05    # damping coefficient\n",
    "\n",
    "def pendulum_acceleration(x, v, t, f_drive, omega):\n",
    "    \"\"\"\n",
    "    Computes the acceleration of the pendulum given:\n",
    "      x: angular displacement (tensor),\n",
    "      v: angular velocity (tensor),\n",
    "      t: current time (float),\n",
    "      f_drive: driving amplitude (float),\n",
    "      omega: driving frequency (float).\n",
    "      \n",
    "    The force is applied as a square wave using sign(sin(omega*t)).\n",
    "    \n",
    "    Note: To ensure torch.sin receives a Tensor instead of a float,\n",
    "    we wrap the scalar omega*t in torch.tensor().\n",
    "    \"\"\"\n",
    "    # Convert omega*t to a torch tensor so that torch.sin can work correctly.\n",
    "    sine_arg = torch.tensor(omega * t, dtype=torch.float)\n",
    "    drive = f_drive * torch.sign(torch.sin(sine_arg))\n",
    "    a = -(g / l) * torch.sin(x) - k_damp * v + drive\n",
    "    return a\n",
    "\n",
    "def rk4_step(x, v, t, dt, f_drive, omega):\n",
    "    \"\"\"\n",
    "    Advances the state (x, v) by one Runge–Kutta 4 step.\n",
    "    \"\"\"\n",
    "    # k1\n",
    "    a1 = pendulum_acceleration(x, v, t, f_drive, omega)\n",
    "    k1_x = v\n",
    "    k1_v = a1\n",
    "\n",
    "    # k2\n",
    "    x2 = x + 0.5 * dt * k1_x\n",
    "    v2 = v + 0.5 * dt * k1_v\n",
    "    a2 = pendulum_acceleration(x2, v2, t + 0.5 * dt, f_drive, omega)\n",
    "    k2_x = v2\n",
    "    k2_v = a2\n",
    "\n",
    "    # k3\n",
    "    x3 = x + 0.5 * dt * k2_x\n",
    "    v3 = v + 0.5 * dt * k2_v\n",
    "    a3 = pendulum_acceleration(x3, v3, t + 0.5 * dt, f_drive, omega)\n",
    "    k3_x = v3\n",
    "    k3_v = a3\n",
    "\n",
    "    # k4\n",
    "    x4 = x + dt * k3_x\n",
    "    v4 = v + dt * k3_v\n",
    "    a4 = pendulum_acceleration(x4, v4, t + dt, f_drive, omega)\n",
    "    k4_x = v4\n",
    "    k4_v = a4\n",
    "\n",
    "    # Combine increments\n",
    "    new_x = x + (dt / 6.0) * (k1_x + 2 * k2_x + 2 * k3_x + k4_x)\n",
    "    new_v = v + (dt / 6.0) * (k1_v + 2 * k2_v + 2 * k3_v + k4_v)\n",
    "    return new_x, new_v\n",
    "\n",
    "def simulate_pendulum(f_drive, omega, num_cycles=5, samples_per_cycle=1000):\n",
    "    \"\"\"\n",
    "    Simulates the forced pendulum for a given number of drive cycles.\n",
    "    Each simulation starts from initial conditions x=0, v=0.\n",
    "    \n",
    "    Parameters:\n",
    "      f_drive: driving amplitude (float)\n",
    "      omega: driving frequency (float)\n",
    "      num_cycles: number of cycles in the simulation\n",
    "      samples_per_cycle: number of reservoir samples per cycle\n",
    "      \n",
    "    Returns:\n",
    "      A 1D torch tensor containing the transient dynamics (the reservoir state).\n",
    "    \"\"\"\n",
    "    period = 2 * math.pi / omega\n",
    "    T = num_cycles * period\n",
    "    total_steps = num_cycles * samples_per_cycle\n",
    "    dt = T / total_steps  # time step\n",
    "\n",
    "    # initial conditions: x = 0, v = 0\n",
    "    x = torch.tensor(0.0)\n",
    "    v = torch.tensor(0.0)\n",
    "    t = 0.0\n",
    "\n",
    "    x_traj = []\n",
    "    for i in range(total_steps):\n",
    "        x, v = rk4_step(x, v, t, dt, f_drive, omega)\n",
    "        t += dt\n",
    "        x_traj.append(x.item())\n",
    "    return torch.tensor(x_traj, dtype=torch.float)\n",
    "\n",
    "def target_polynomial(x):\n",
    "    \"\"\"\n",
    "    Computes the seventh-degree polynomial:\n",
    "      f(x) = (x-3)(x-2)(x-1)x(x+1)(x+2)(x+3)\n",
    "    \"\"\"\n",
    "    return (x - 3) * (x - 2) * (x - 1) * x * (x + 1) * (x + 2) * (x + 3)\n",
    "\n",
    "def build_dataset(num_samples, num_cycles=5, samples_per_cycle=1000, encoding='frequency'):\n",
    "    \"\"\"\n",
    "    Constructs a dataset for Task I.\n",
    "    \n",
    "    For amplitude encoding, each input u (sampled uniformly from [-3,3])\n",
    "    is encoded by setting the driving amplitude as:\n",
    "      f_drive = 1 + (u + 3) / 6.\n",
    "    The target is the polynomial value at u.\n",
    "    \n",
    "    Returns:\n",
    "      u: original input values (shape: [num_samples])\n",
    "      reservoir_matrix: reservoir states (shape: [num_samples, reservoir_dimension])\n",
    "      y_target: target polynomial values (shape: [num_samples])\n",
    "    \"\"\"\n",
    "    # Generate inputs u in [-3, 3]\n",
    "    u = 6 * torch.rand(num_samples) - 3\n",
    "    y_target = target_polynomial(u)\n",
    "    \n",
    "    if encoding == 'amplitude':\n",
    "        # Map input u to driving amplitude f_drive in [1, 2]\n",
    "        f_drive_vals = 1 + (u + 3) / 6\n",
    "        omega_vals = torch.ones_like(u)  # constant driving frequency (e.g., 1.0)\n",
    "    elif encoding == 'frequency':\n",
    "        # Alternatively, for frequency encoding (not used in this example)\n",
    "        f_drive_vals = torch.ones_like(u)\n",
    "        omega_vals = 0.5 + (u + 3) / 6  # example: omega in [0.5, 1.5]\n",
    "    else:\n",
    "        raise ValueError(\"Encoding must be 'amplitude' or 'frequency'\")\n",
    "    \n",
    "    reservoir_states = []\n",
    "    for i in range(num_samples):\n",
    "        # Convert the encoded values to native Python numbers.\n",
    "        f_drive = f_drive_vals[i].item()\n",
    "        omega = omega_vals[i].item()\n",
    "        x_traj = simulate_pendulum(f_drive, omega, num_cycles, samples_per_cycle)\n",
    "        reservoir_states.append(x_traj)\n",
    "    reservoir_matrix = torch.stack(reservoir_states, dim=0)\n",
    "    return u, reservoir_matrix, y_target\n",
    "\n",
    "class LinearReadout(torch.nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LinearReadout, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "def train_linear_readout(train_features, train_targets, num_epochs=500, learning_rate=1e-3):\n",
    "    num_samples, input_dim = train_features.shape\n",
    "    model = LinearReadout(input_dim)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    train_targets = train_targets.view(-1, 1)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(train_features)\n",
    "        loss = criterion(outputs, train_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.6f}\")\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    torch.manual_seed(0)  \n",
    "\n",
    "    num_train = 500\n",
    "    u_train, train_features, y_train = build_dataset(num_train, num_cycles=10, samples_per_cycle=100, encoding='amplitude')\n",
    "    print(\"Training dataset built. Reservoir state dimension:\", train_features.shape[1])\n",
    "    \n",
    "    model = train_linear_readout(train_features, y_train, num_epochs=500, learning_rate=1e-3)\n",
    "    \n",
    "    num_test = 100\n",
    "    u_test, test_features, y_test = build_dataset(num_test, num_cycles=5, samples_per_cycle=100, encoding='amplitude')\n",
    "    \n",
    "    # Evaluate the trained model on the test set using MSE loss\n",
    "    with torch.no_grad():\n",
    "        predictions = model(test_features).squeeze()\n",
    "        test_loss = torch.mean((predictions - y_test) ** 2).item()\n",
    "    print(f\"Test Loss: {test_loss:.6f}\")\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.scatter(u_test.numpy(), y_test.numpy(), label=\"Target\", color=\"blue\")\n",
    "    plt.scatter(u_test.numpy(), predictions.numpy(), label=\"Prediction\", color=\"red\")\n",
    "    plt.xlabel(\"Input u (in [-3, 3])\")\n",
    "    plt.ylabel(\"Polynomial Value\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Reservoir Computing Performance on Polynomial Approximation\")\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
