@article{article_RC_intro,
author = {Cucchi, Matteo and Abreu, Steven and Ciccone, Giuseppe and Brunner, Daniel and Kleemann, Hans},
year = {2022},
month = {08},
pages = {},
title = {Hands-on reservoir computing: a tutorial for practical implementation},
volume = {2},
journal = {Neuromorphic Computing and Engineering},
doi = {10.1088/2634-4386/ac7db7}
}


@unknown{hydrodynamics_new_rc,
author = {Marcucci, Giulia and Caramazza, Piergiorgio and Shrivastava, Shamit},
year = {2023},
month = {02},
pages = {},
title = {A New Paradigm of Reservoir Computing Exploiting Hydrodynamics},
doi = {10.48550/arXiv.2302.01978}
}

@article{article_esn_intro,
author = {Jaeger, Herbert},
year = {2001},
month = {01},
pages = {},
title = {The" echo state" approach to analysing and training recurrent neural networks-with an erratum note'},
volume = {148},
journal = {Bonn, Germany: German National Research Center for Information Technology GMD Technical Report}
}

@article{article_lsm_intro,
author = {Maass, Wolfgang},
year = {2010},
month = {01},
pages = {},
title = {Liquid State Machines: Motivation, Theory, and Applications},
isbn = {978-1-84816-245-7},
doi = {10.1142/9781848162778_0008}
}

@article{article_catch_22s_rc,
author = {Zhang, Yuanzhao and Cornelius, Sean},
year = {2023},
month = {09},
pages = {},
title = {Catch-22s of reservoir computing},
volume = {5},
journal = {Physical Review Research},
doi = {10.1103/PhysRevResearch.5.033213}
}


@article{Arun2024,
  author = {Arun, R. and Aravindh, M. S. and Venkatesan, A. and Lakshmanan, M.},
  title = {Reservoir computing with logistic map},
  journal = {arXiv preprint arXiv:2401.09501},
  year = {2024}
}
@article{Mandal2022,
  author = {Mandal, Swarnendu and Sinha, Sudeshna and Shrimali, Manish Dev},
  title = {Machine-learning potential of a single pendulum},
  journal = {Physical Review E},
  volume = {105},
  pages = {054203},
  year = {2022},
  doi = {10.1103/PhysRevE.105.054203}
}
@article{Itoh2020,
  author = {Itoh, Y. and Uenohara, S. and Adachi, M. and Morie, T. and Aihara, K.},
  title = {Reconstructing bifurcation diagrams only from time-series data generated by electronic circuits in discrete-time dynamical systems},
  journal = {Chaos},
  volume = {30},
  number = {1},
  pages = {013128},
  year = {2020},
  doi = {10.1063/1.5119187}
}


 @misc{ wiki:esn,
   author = "Wikimedia Commons",
   title = "File:FreqGenSchema.png --- Wikimedia Commons{,} the free media repository",
   year = "2024",
   url = "https://commons.wikimedia.org/w/index.php?title=File:FreqGenSchema.png&oldid=955422627",
   note = "[Online; accessed 26-April-2025]"
 }

@article{WOO2024129334,
title = {Characterization of the neuronal and network dynamics of liquid state machines},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {633},
pages = {129334},
year = {2024},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2023.129334},
url = {https://www.sciencedirect.com/science/article/pii/S0378437123008890},
author = {Junhyuk Woo and Soon Ho Kim and Hyeongmo Kim and Kyungreem Han},
keywords = {Reservoir computing (RC), Liquid state machine (LSM), Neuronal avalanche, Network clustering, Scale invariance, Self-organized criticality (SOC)},
abstract = {Reservoir computing (RC) is a relatively new machine-learning framework that uses an abstract neural network model, called reservoir. The reservoir forms a complex system with high dimensionality, nonlinearity, and intrinsic memory effect due to recurrent connections among individual neurons. RC manifests a best-in-class performance in processing information generated by complex dynamical systems, yet little is known about its microscopic/macroscopic dynamics underlying the computational capability. Here, we characterize the neuronal and network dynamics of liquid state machines (LSMs) using numerical simulations and Modified National Institute of Standards and Technology (MNIST) database classification tasks. The computational performance of LSMs largely depends on a dynamic range of neuronal avalanches whereby the avalanche patterns are determined by the neuron and network models. A larger dynamic range leads to higher performance—the MNIST classification accuracy is highest when the avalanche sizes follow a slowly decaying power-law distribution with an exponent of ∼1.5, followed by the power-law statistics with a larger exponent and the mixture of power-law/log-normal distributions. Network-theoretic analysis suggests that the formation of large functional clusters and the promotion of dynamic transitions between large and small clusters may contribute to the scale-invariant nature. This study provides new insight into our understanding of the computational principles of RC concerning the actions of the individual neurons and the system-level collective behavior.}
}



@misc{gu2024mambalineartimesequencemodeling,
      title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces}, 
      author={Albert Gu and Tri Dao},
      year={2024},
      eprint={2312.00752},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2312.00752}, 
}


